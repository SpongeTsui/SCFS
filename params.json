{"name":"SCFS","tagline":"\"Shared Cloud-backed File System\"","body":"SCFS is a cloud-backed file system that provides strong consistency even on top of eventually-consistent cloud storage services. It is build on top of [FUSE](http://fuse.sourceforge.net/), thus providing a POSIX-like interface.\r\nSCFS provides also a pluggable backend that allows it to work with a single cloud or with a cloud-of-clouds.\r\nThe design, implementation and evaluation of the system is described in our [USENIX ATC'14 paper](http://www.di.fc.ul.pt/~bessani/publications/usenix14-scfs.pdf).\r\n\r\nIts main concern is address some of the problems presented on the other existing cloud-backed file systems:\r\n\r\n1. Most of them not support controlled file sharing among clients.\r\n2. Some, use a proxy to connect with the cloud storage, which is a single point of failure, once if it is down, no client can access to the data stored at the cloud side.\r\n3. Almost all file systems use only a single cloud as backend.\r\n\r\n## Architecture\r\nThe figure bellow presents the architecture of SCFS with its three main components: the backend cloud storage for maintaining the file data; the coordination service for managing the metadata and to support synchronization; and the SCFS Agent that implements most of the SCFS functionality, and corresponds to the file system client mounted at the user machine.\r\n\r\n`<FIGURE>`\r\n\r\nThe figure shows the backend cloud storage as cloud-of-clouds formed by Amazon S3, Google Storage, RackSpace Cloud Files and Windows Azure. To store data in this clouds, SCFS uses DepSky. More specifically, uses DepSky-CA protocol once we want address the main cloud storage limitations (see DepSky). SCFS allows also a different backend formed by only one cloud.\r\n\r\nThe coordination services used by SCFS are DepSpace and ZooKepper. In the figure above they are installed in computing clouds, but they can be installed on any IaaS or in any server. To get more information about theses system please read the referred papers. [DepSpace](https://github.com/bft-smart/depspace) is described on a [EuroSys'08 paper](http://www.di.fc.ul.pt/~bessani/publications/eurosys08-depspace.pdf) and ZooKepper on a [Usenix'10 paper](https://www.usenix.org/legacy/event/usenix10/tech/full_papers/Hunt.pdf).\r\n\r\nThe SCFS Agent component is the one that implements the most features of the file system. It makes a lot of usage of cache techniques to improve the system performance, to reduce the monetary costs and to increase the data availability. More Specifically it uses a temporary small memory cache to absorb some metadata update bursts, a main memory cache to maintain data of open files, and a bigger disk cache to save all the most recents files used by the client. The last one uses all the free space in disk. After the disk is full it uses LRU policies to create new space.\r\n\r\n## Operation Modes\r\nSCFS is a very configurable file system, once clients can use it according with their needs. There is four main configurations that change both the file system behaviour as the provided guarantees:\r\n\r\nThe system can be configured to use only one cloud or a cloud-of-clouds for both the backend cloud storage as to the coordination service.\r\nThe client can choose if it uses the main memory cache to store open files data, or not.\r\nThe sending of data to the clouds can be synchronous or asynchronous.\r\nThe system can be sharing or non-sharing. If non-sharing configuration is used the system does not uses none coordination service. Is considered that all files are private (not shared), therefore all the metadata can be stored in the storage clouds.\r\n\r\n***\r\n## Getting Started with SCFS\r\n\r\nHere you can find how to configure, install and use the available version of SCFS. The version we provide uses DepSpace as coordination service and Amazon S3 as single cloud storage backend. \r\n\r\nThe first step is downloadd the latest version available of SCFS. After that you need to extract the archive downloaded. Make sure you have java 1.7 or later installed.\r\n\r\nNext, you need to link the system with the Fuse on your machine. To do that, you just need to run a script we provide in the project's root directory. If you have a x64 Java installed, you just need to run:\r\n\r\n`./configure_x64.sh`\r\n\r\nIn the other hand, if your java is a x86 version, please run:\r\n\r\n`./configure_x86.sh`\r\n\r\nIn the next step you just need to set your JAVA_HOME and FUSE_HOME in the build.conf file. Usually, your fuse home will be in /usr/local. The JAVA_HOME will depend on the java version you have installed on your machine.\r\n\r\nThe next step is to configure the remain configuration files. More specifically, it is necessary to fill configuration files for DepSky and DepSpace. The following subsections explain how to do it.\r\n\r\n### Setting up DepSky\r\nTo fill DepSky configuration files please read the _'Getting Started with DepSky'_ section in DepSky page. Here you can learn how to create the cloud storage accounts and where you can find the necessary API keys.\r\n\r\nIf you want to share files with other users, you have also to fill the canonicalId field in each entry of the accounts.properties. The version avilable of the system only allows sharing files between different accounts if you use only Amazon S3 (both in one single cloud and cloud-of-clouds backends). You can find your canonical Id in the Security Credentials page of your AWS Management Console.\r\n\r\nYou can use also SCFS with only one cloud storage as backend. For now, we only support Amazon S3. To configure it you just need to delete the last three entries in the account.properties file (see HowToUseDepSky). Your file must be equal to the content below:\r\n\r\n```\r\n#AMAZON\r\ndriver.type=AMAZON-S3\r\ndriver.id=cloud1\r\naccessKey=********************\r\nsecretKey=****************************************\r\nlocation=EU_Ireland\r\ncanonicalId=******************************************************\r\n```\r\n\r\n### Setting up DepSpace\r\n\r\n[DepSpace](https://github.com/bft-smart/depspace) configuration consists in two main steps; the address and port configuration of all DepSpace's replicas, and the configuration of system parameters. This could be done in config/hosts.config and config/system.config files respectively. To comment out a configuration parameter in those files, the line must start with the character #.\r\n\r\nDepSpace is built on top of [BFT-SMaRt](https://github.com/bft-smart/library). Please take a look at _How to run BFT-SMaRt_.\r\n\r\nIn _hosts.config_ file you can set the address and port of all DepSpace replicas. The configuration file should look like this:\r\n\r\n```\r\n#server id, address and port (the ids from 0 to n-1 are the service replicas) \r\n0 127.0.0.1 11000\r\n1 127.0.0.1 11010\r\n2 127.0.0.1 11020\r\n3 127.0.0.1 11030\r\n4 127.0.0.1 11040\r\n5 127.0.0.1 11050\r\n6 127.0.0.1 11060\r\n7 127.0.0.1 11070\r\n```\r\n\r\nAfter the configuration of the replicas address and port, you can set the parameter Besides the parameters you need to set to BFT-SMaRt, there are others you need to configure to DepSpace. These parameters are described below:\r\n\r\nParameter | Description | Values | Default value\r\n----------|-------------|--------|--------------\r\nsystem.general.rdRetryTime | The time the system blocks on rd operation waiting for a match tuple in the space. SCFS does not uses this operation. | Integer (miliseconds) |10000\r\nsystem.general.realTimeRenew | Use or not of renewable tuples. Note that SCFS needs this parameter to be true. | true or false | true\r\nsystem.general.logging | Use or not of logging. Set this parameter to true if you want it. | true or false | false\r\nsystem.general.logging.mode | Set the logging mode. The modes are: 0=(no logging), 1=\"rw\", 2=\"rws\", (any other)=\"rwd\" | Integer | 0\r\nsystem.general.batching | Use batches of messages. We recommend you to set this to false. | true or false | false\r\nsystem.general.checkpoint_period | The period (number of operation) in which the system performs a checkpoint of its state. | Integer | 1000\r\nsystem.general.replaceTrigger | The usage or not of the DepSpace's replace trigger layer. Note that SCFS needs this layer to work correctly. | true or false | true\r\n\r\nWe recommend you to use the default values set in the _config/system.config_ file available.\r\n\r\n#### Public Keys\r\nWe provide some default DepSpace keys. However, if you need to generate new public/private keys for DepSpace replicas or clients, you can use the following command to generate files for public and private keys:\r\n\r\n`./rsaKeyPairGenerator.sh <id of the replica/client>`\r\n\r\nKeys are stored in the _config/keys_ folder. The command above creates key pairs both for clients and replicas. Currently such keys need to be manually distributed.\r\n\r\n#### Using DepSpace with just 1 replica\r\nThe default _system.config_ and _hosts.config_ files are set to use Byzantine Fault tolerant DepSpace (using 3f+1 replicas).\r\n\r\nTo use DepSpace without any fault tolerance guarantee, some modification need to be done in the _config/system.config_ file. In this way you need to set the following parameters of this file:\r\n\r\n```\r\n#Number of servers in the group\r\nsystem.servers.num = 1\r\n```\r\n```\r\n#Maximum number of faulty replicas \r\nsystem.servers.f = 0\r\n```\r\n```\r\n#Initial View \r\nsystem.initial.view = 0\r\n```\r\n\r\nYou also need to reconfigure the replicas' address/port. To do that, you should update your _config/hosts.config_ file to look like the following:\r\n\r\n```\r\n#server id, address and port (the ids from 0 to n-1 are the service replicas) \r\n0 127.0.0.1 11000\r\n```\r\n\r\n### Deploying DepSpace\r\nTo deploy DepSpace in a server (being it in the clouds or in other location), you first need to get the files to deploy. This files can be generated by executing the command bellow:\r\n\r\n`./generateDepSpaceToDeploy.sh`\r\n\r\nThis script will generate a zip file (DepSpace.zip) with the configurations you did for the DepSpace locally (namely the _hosts.config_ file). To deploy DepSpace other sites, you just need to upload de generated file to these sites and unzip it:\r\n\r\n`unzip DepSpace.zip`\r\n\r\nAfter unzip the file, to run the DepSpace in each site, you just need to run the _runDepSpace.sh_ script in each site. You can find this script inside the DepSpace folder after you unzip the file. To execute this script you should run the follow command:\r\n\r\n`./runDepSpace.sh <replicaID>`\r\n\r\n### Mounting SCFS\r\nTo mount SCFS you just need to run the following command in the root of the project:\r\n\r\n`./mountSCFS.sh <mountPoint> <clientID>`\r\n\r\nThe meaning of the two given arguments are:\r\n\r\n* _mountPoint_ - the folder where SCFS will be mounted;\r\n\r\n* _clientID_ - the identifier of the client (must be unique).\r\n\r\nEach time you mount the system you are able to define how the system will behave, opting for several parameters. This parameters will have influence in the system guarantees and performance. Note that the default behavior of the system (the command presented before) provides the highest level of guarantees and the lower performance.\r\n\r\nIn the table bellow are shown all the flags that can be turned on:\r\n\r\nParameter             | Description | Default value\r\n----------------------|-------------|--------------\r\n-optimizedCache | Use of main memory data cache. | off\r\n--sync-to-cloud\t| The FSYNC POSIX operation flushes the data to the clouds. If turned off, FSYNC flushes data to local disk. | off\r\n--non-blocking-cloud | Data is sent to the clouds asynchronously. If turned off, the file close operation will block when data are sent to the clouds. | off\r\n--non-sharing | The system runs in non-sharing mode of operation. It means that clients are not able to share data between them | off\r\n-printer | Turn on the debug mode. | off\r\n\r\nAs you can understand, if the --non-blocking-cloud flag is turned on, the performance of the system is increased while the guarantees offered by the close operation decrease.\r\n\r\n#### Non-Sharing SCFS\r\nTo use this operation mode, you should turn on the _--non-sharing_ flag as shown bellow:\r\n\r\n`./mountSCFS.sh <mountPoint> <clientID> --non-sharing`\r\n\r\nThis operation mode is ideal to use the system with its highest performance, backuping the file system data to the clouds. So, the use of _--non-blocking-cloud_ flag are recommended:\r\n\r\n`./mountSCFS.sh <mountPoint> <clientID> --non-sharing --non-blocking-cloud`\r\n\r\nNote: Since all files are personal in this operation mode, it does not need the DepSpace deployment. So, if you want to try SCFS without experience the deployment effort of DepSpace, this is the perfect option to do that.\r\n\r\n## Using SCFS\r\nTo use SCFS you just need to enter in the _<mountPoint>_ folder and operate in the same way you do in your local filesystem. For example, let your SCFS configuration be:\r\n\r\n`./mountSCFS.sh /scfs 4 --non-sharing --non-blocking-cloud -optimizedCache`\r\n\r\nIn the example, your _<mountPoint>_ will be _/scfs_. So, you just need to operate over the files in that folder to take advantage of SCFS.\r\n\r\n#### Share a file using SCFS\r\nTo share a file, you just need to use the setfacl command, giving the clientId of the client with which you want to share a file, and the permissions to that client:\r\n\r\n`setfacl -m u:<friendClientID>:<permissions> <fileName>`\r\n\r\nFor example, assuming there is a file named _file1.txt_, to share this file with the client 5 giving full control over the file, you just need to run the following command:\r\n\r\n`setfacl -m u:5:rwx file1.txt`","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}